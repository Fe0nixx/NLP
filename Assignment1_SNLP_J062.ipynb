{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "Ae5UZ4SRKhWD",
        "outputId": "76c39e43-cfce-43a1-c634-5f6f7201f3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "0994bc84a22442e3a45525c5e8d392d0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install gensim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1**"
      ],
      "metadata": {
        "id": "kwBSMUSeOpKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import warnings\n",
        "\n",
        "# Suppress deprecation warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "def perform_word2vec_operations():\n",
        "    \"\"\"\n",
        "    Loads the 'word2vec-google-news-300' model and demonstrates finding\n",
        "    similar words and solving word analogies.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the pre-trained Word2Vec model.\n",
        "        # This will download the model if it's not already cached (approx. 1.6GB).\n",
        "        print(\"Loading the 'word2vec-google-news-300' model...\")\n",
        "        print(\"This may take a few minutes if you are downloading it for the first time...\")\n",
        "        model = api.load('word2vec-google-news-300')\n",
        "        print(\"Model loaded successfully.\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load the model. Error: {e}\")\n",
        "        print(\"Please ensure you have a stable internet connection for the download.\")\n",
        "        return\n",
        "\n",
        "    # --- Part 1: Finding Similar Words ---\n",
        "    print(\"### Part 1: Finding Similar Words ###\\n\")\n",
        "\n",
        "    words_to_check = ['guitar', 'France', 'programming', 'sunshine', 'coffee']\n",
        "\n",
        "    for word in words_to_check:\n",
        "        try:\n",
        "            # Find the top 5 most similar words\n",
        "            similar_words = model.most_similar(word, topn=5)\n",
        "            print(f\"Words most similar to '{word}':\")\n",
        "            for similar_word, score in similar_words:\n",
        "                # Print the similar word and its cosine similarity score\n",
        "                print(f\"- {similar_word} (Score: {score:.2f})\")\n",
        "            print(\"-\" * 20)\n",
        "        except KeyError:\n",
        "            print(f\"The word '{word}' is not in the model's vocabulary.\")\n",
        "        print()\n",
        "\n",
        "\n",
        "    # --- Part 2: Word Analogy Experiments ---\n",
        "    print(\"\\n### Part 2: Word Analogy Experiments ###\\n\")\n",
        "\n",
        "    def test_analogy(positive_words, negative_words):\n",
        "        \"\"\"\n",
        "        Calculates and prints the result of a word analogy.\n",
        "        Analogy: positive[0] is to negative[0] as result is to positive[1].\n",
        "        Example: king - man + woman = queen\n",
        "        \"\"\"\n",
        "        equation = f\"{positive_words[0]} - {negative_words[0]} + {positive_words[1]}\"\n",
        "        try:\n",
        "            # Perform the vector calculation for the analogy\n",
        "            result = model.most_similar(positive=positive_words, negative=negative_words, topn=1)\n",
        "            print(f\"Analogy: {equation} ~= {result[0][0]} (Score: {result[0][1]:.2f})\")\n",
        "        except KeyError as e:\n",
        "            print(f\"Could not perform analogy '{equation}' because the word {e} is not in the vocabulary.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred with the analogy '{equation}': {e}\")\n",
        "\n",
        "\n",
        "    # Analogy 1: king - man + woman ~= queen\n",
        "    print(\"--- Analogy 1: Royalty and Gender ---\")\n",
        "    test_analogy(positive_words=['king', 'woman'], negative_words=['man'])\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Analogy 2: France - Paris + Berlin ~= Germany\n",
        "    print(\"--- Analogy 2: Country and Capital ---\")\n",
        "    test_analogy(positive_words=['France', 'Berlin'], negative_words=['Paris'])\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Analogy 3: walking - ran + swam ~= walked (Testing verb tenses)\n",
        "    print(\"--- Analogy 3: Verb Tense ---\")\n",
        "    # Note: 'walking' and 'swam' are used to find the past tense of 'walk'.\n",
        "    # A better analogy might be 'running' - 'ran' + 'walked' to get 'walking'. Let's test the original idea.\n",
        "    test_analogy(positive_words=['walking', 'swam'], negative_words=['ran'])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    perform_word2vec_operations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbrr_ZRiKiCM",
        "outputId": "45737532-4fa0-4fb2-f0c5-95fbaef7ba0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the 'word2vec-google-news-300' model...\n",
            "This may take a few minutes if you are downloading it for the first time...\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Model loaded successfully.\n",
            "\n",
            "### Part 1: Finding Similar Words ###\n",
            "\n",
            "Words most similar to 'guitar':\n",
            "- guitars (Score: 0.84)\n",
            "- acoustic_guitar (Score: 0.83)\n",
            "- electric_guitar (Score: 0.81)\n",
            "- mandolin (Score: 0.78)\n",
            "- harmonica (Score: 0.77)\n",
            "--------------------\n",
            "\n",
            "Words most similar to 'France':\n",
            "- French (Score: 0.70)\n",
            "- extradites_Noriega (Score: 0.69)\n",
            "- Belgium (Score: 0.69)\n",
            "- Villebon_Sur_Yvette (Score: 0.68)\n",
            "- PARIS_AFX_Gaz_de (Score: 0.66)\n",
            "--------------------\n",
            "\n",
            "Words most similar to 'programming':\n",
            "- programing (Score: 0.86)\n",
            "- Programming (Score: 0.69)\n",
            "- NLP_neuro_linguistic (Score: 0.62)\n",
            "- broadcasts (Score: 0.60)\n",
            "- primetime_programming (Score: 0.60)\n",
            "--------------------\n",
            "\n",
            "Words most similar to 'sunshine':\n",
            "- sunny (Score: 0.73)\n",
            "- sun (Score: 0.68)\n",
            "- bright_sunshine (Score: 0.64)\n",
            "- blue_skies (Score: 0.63)\n",
            "- autumnal_sunshine (Score: 0.63)\n",
            "--------------------\n",
            "\n",
            "Words most similar to 'coffee':\n",
            "- coffees (Score: 0.72)\n",
            "- gourmet_coffee (Score: 0.71)\n",
            "- Coffee (Score: 0.69)\n",
            "- o_joe (Score: 0.69)\n",
            "- Starbucks_coffee (Score: 0.69)\n",
            "--------------------\n",
            "\n",
            "\n",
            "### Part 2: Word Analogy Experiments ###\n",
            "\n",
            "--- Analogy 1: Royalty and Gender ---\n",
            "Analogy: king - man + woman ~= queen (Score: 0.71)\n",
            "\n",
            "\n",
            "--- Analogy 2: Country and Capital ---\n",
            "Analogy: France - Paris + Berlin ~= Germany (Score: 0.79)\n",
            "\n",
            "\n",
            "--- Analogy 3: Verb Tense ---\n",
            "Analogy: walking - ran + swam ~= swimming (Score: 0.61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2**"
      ],
      "metadata": {
        "id": "CrbJ5SvUPHpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkqpDiOqPTnV",
        "outputId": "25b84efa-af91-4a4d-be23-d3c3aac896df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "# Load dataset\n",
        "df = pd.read_csv(os.path.join(path, \"IMDB Dataset.csv\"))\n",
        "\n",
        "\n",
        "# Basic info\n",
        "print(df.info())\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Sample text length statistics\n",
        "df['text_length'] = df['review'].apply(len)\n",
        "print(df['text_length'].describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5pLUrduKtbm",
        "outputId": "a4c71218-415b-4f2d-b19a-bf8ac07d07b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n",
            "None\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "count    50000.000000\n",
            "mean      1309.431020\n",
            "std        989.728014\n",
            "min         32.000000\n",
            "25%        699.000000\n",
            "50%        970.000000\n",
            "75%       1590.250000\n",
            "max      13704.000000\n",
            "Name: text_length, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())        # Remove punctuation & lowercase\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg0HXg_8PLVO",
        "outputId": "772519d4-68b9-41e9-f904-fa047541c4fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/NathaNn1111/word2vec-google-news-negative-300-bin/resolve/main/GoogleNews-vectors-negative300.bin -O GoogleNews-vectors-negative300.bin\n",
        "\n",
        "# üß† Load the model using gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "w2v_model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "\n",
        "# ‚úÖ Test it\n",
        "print(w2v_model[\"movie\"])  # Outputs the vector for the word \"movie\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzfw7cZnQzbo",
        "outputId": "91005748-1329-4950-b081-bc305a5c25db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-20 05:19:35--  https://huggingface.co/NathaNn1111/word2vec-google-news-negative-300-bin/resolve/main/GoogleNews-vectors-negative300.bin\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.106, 3.168.73.111, 3.168.73.129, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/da/6d/da6dfa830175be54eb7731878dedb830df17d2d54f225a41fc904158015f4fa0/6cb30ecf940501bdb0a0b2921562d4008ab3e461c342b44b48f801211f9c0f8e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GoogleNews-vectors-negative300.bin%3B+filename%3D%22GoogleNews-vectors-negative300.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1752992375&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mjk5MjM3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2RhLzZkL2RhNmRmYTgzMDE3NWJlNTRlYjc3MzE4NzhkZWRiODMwZGYxN2QyZDU0ZjIyNWE0MWZjOTA0MTU4MDE1ZjRmYTAvNmNiMzBlY2Y5NDA1MDFiZGIwYTBiMjkyMTU2MmQ0MDA4YWIzZTQ2MWMzNDJiNDRiNDhmODAxMjExZjljMGY4ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=CznbZxBsGGFmhxSgoe-eRy5O%7E8gZIP5zBX-ieg7CTtcm%7E1OyBGNe8wKQWRmmC1qGT27IJMqtC3NYfdcK64ya9WcOUPBdK56dzbfW1KstvI8wWoGuvbCNVwPCUcsM0P-8LvCBTwi0C6F2ZWvMJnmKq9TcHvRptb0zKINzkgekbKra6DeN0RjIvMvZDfwQaSAra8GWICKLmljiJjkZg8TbACiSqOm49WZZqIj3k6JfZW6k-P6Qe7ZoygDBIhk8PXDPILrUt%7E72SIM4cSYo9xVt5R3H-jNRwkAj--uDwmCTNHdGot-GHtGjDhPhXqdyMHd0BCM1h0xpXP6lBwCSNjbU5w__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-07-20 05:19:35--  https://cdn-lfs-us-1.hf.co/repos/da/6d/da6dfa830175be54eb7731878dedb830df17d2d54f225a41fc904158015f4fa0/6cb30ecf940501bdb0a0b2921562d4008ab3e461c342b44b48f801211f9c0f8e?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GoogleNews-vectors-negative300.bin%3B+filename%3D%22GoogleNews-vectors-negative300.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1752992375&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1Mjk5MjM3NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2RhLzZkL2RhNmRmYTgzMDE3NWJlNTRlYjc3MzE4NzhkZWRiODMwZGYxN2QyZDU0ZjIyNWE0MWZjOTA0MTU4MDE1ZjRmYTAvNmNiMzBlY2Y5NDA1MDFiZGIwYTBiMjkyMTU2MmQ0MDA4YWIzZTQ2MWMzNDJiNDRiNDhmODAxMjExZjljMGY4ZT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=CznbZxBsGGFmhxSgoe-eRy5O%7E8gZIP5zBX-ieg7CTtcm%7E1OyBGNe8wKQWRmmC1qGT27IJMqtC3NYfdcK64ya9WcOUPBdK56dzbfW1KstvI8wWoGuvbCNVwPCUcsM0P-8LvCBTwi0C6F2ZWvMJnmKq9TcHvRptb0zKINzkgekbKra6DeN0RjIvMvZDfwQaSAra8GWICKLmljiJjkZg8TbACiSqOm49WZZqIj3k6JfZW6k-P6Qe7ZoygDBIhk8PXDPILrUt%7E72SIM4cSYo9xVt5R3H-jNRwkAj--uDwmCTNHdGot-GHtGjDhPhXqdyMHd0BCM1h0xpXP6lBwCSNjbU5w__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.167.56.117, 3.167.56.123, 3.167.56.101, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.167.56.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3644258522 (3.4G) [application/octet-stream]\n",
            "Saving to: ‚ÄòGoogleNews-vectors-negative300.bin‚Äô\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   3.39G   204MB/s    in 32s     \n",
            "\n",
            "2025-07-20 05:20:07 (107 MB/s) - ‚ÄòGoogleNews-vectors-negative300.bin‚Äô saved [3644258522/3644258522]\n",
            "\n",
            "[ 0.17480469 -0.10986328 -0.20019531  0.26757812 -0.06396484  0.06689453\n",
            "  0.07958984  0.08398438  0.12695312  0.11621094  0.11523438 -0.13867188\n",
            " -0.08203125 -0.00143433 -0.19824219  0.13574219 -0.03955078  0.06933594\n",
            " -0.2265625  -0.20019531  0.03076172  0.16015625 -0.04174805  0.00427246\n",
            "  0.09619141 -0.03320312  0.02783203  0.02124023  0.13867188 -0.02075195\n",
            " -0.31835938 -0.08837891 -0.23828125  0.02490234  0.06787109 -0.18066406\n",
            "  0.27148438  0.16210938  0.04614258  0.20410156  0.22949219 -0.03710938\n",
            "  0.140625    0.12890625 -0.22558594  0.03857422 -0.01300049  0.00582886\n",
            "  0.23144531  0.1015625  -0.10351562 -0.10351562 -0.2578125   0.16503906\n",
            "  0.03686523 -0.32421875  0.02893066 -0.11914062 -0.19238281  0.00086594\n",
            "  0.06591797  0.265625   -0.15917969  0.26171875 -0.18359375  0.13085938\n",
            " -0.25       -0.05541992  0.27929688 -0.06103516 -0.05322266  0.07470703\n",
            " -0.24609375  0.203125   -0.23925781  0.00634766  0.10742188  0.0324707\n",
            "  0.19921875  0.0456543  -0.04052734 -0.11181641 -0.04956055 -0.203125\n",
            " -0.16503906  0.1796875  -0.15429688  0.15625     0.13671875 -0.09277344\n",
            "  0.00610352  0.09765625  0.08056641  0.05493164 -0.06982422  0.01013184\n",
            "  0.07861328 -0.08203125 -0.10986328 -0.01721191  0.15234375  0.21191406\n",
            "  0.1796875  -0.1875      0.23828125  0.06347656 -0.10253906 -0.2890625\n",
            "  0.10351562 -0.27539062 -0.125       0.06738281  0.11181641 -0.16015625\n",
            "  0.35742188  0.03344727 -0.00842285 -0.15722656 -0.01757812  0.03637695\n",
            " -0.06347656 -0.12402344  0.15917969 -0.05151367 -0.04833984  0.03710938\n",
            " -0.21972656  0.15039062 -0.30664062 -0.01123047 -0.09765625  0.01855469\n",
            "  0.10107422  0.06738281  0.15722656 -0.00521851  0.08349609  0.03271484\n",
            " -0.21875    -0.25585938  0.24316406 -0.09960938  0.02270508  0.18359375\n",
            "  0.05859375 -0.19238281  0.328125    0.32421875 -0.0456543   0.0546875\n",
            " -0.03515625  0.03564453 -0.08935547  0.0625     -0.04125977 -0.32226562\n",
            " -0.14160156 -0.08740234 -0.17773438 -0.0222168   0.05566406 -0.07470703\n",
            " -0.03588867  0.33789062 -0.25        0.01385498  0.21679688  0.04833984\n",
            " -0.08642578  0.03393555  0.02661133 -0.12451172  0.28710938 -0.27734375\n",
            " -0.19726562 -0.00387573  0.33007812 -0.03442383 -0.07080078  0.109375\n",
            "  0.0402832   0.02954102 -0.1171875  -0.24121094  0.24804688  0.05517578\n",
            " -0.04443359  0.07275391  0.02587891  0.06201172  0.10595703  0.18457031\n",
            " -0.20996094  0.24316406  0.09912109  0.14941406 -0.07373047  0.01672363\n",
            " -0.20703125  0.02075195  0.24511719  0.09814453 -0.09912109  0.07861328\n",
            "  0.05639648 -0.13085938 -0.04418945 -0.00382996 -0.43359375  0.02050781\n",
            " -0.03271484  0.51953125  0.10058594 -0.02087402 -0.19140625  0.29882812\n",
            " -0.10791016 -0.00909424 -0.15917969 -0.10058594  0.04223633  0.09912109\n",
            "  0.13964844 -0.02050781  0.18945312  0.15136719 -0.08007812  0.03173828\n",
            " -0.11474609 -0.0859375   0.3359375  -0.23242188 -0.21679688 -0.04638672\n",
            "  0.15136719  0.23925781  0.00927734  0.04296875  0.26953125 -0.11474609\n",
            "  0.20507812 -0.09960938  0.13085938 -0.13183594 -0.12695312 -0.07714844\n",
            "  0.21777344 -0.02111816  0.04321289 -0.01855469 -0.00185394  0.00546265\n",
            "  0.11669922  0.09863281  0.12890625  0.0045166   0.08886719 -0.21484375\n",
            " -0.3203125  -0.2890625  -0.33398438 -0.02502441 -0.07519531 -0.15722656\n",
            "  0.07861328  0.15136719 -0.05834961 -0.1640625  -0.07568359 -0.04296875\n",
            "  0.00294495  0.05200195  0.04223633 -0.02612305  0.25976562  0.11669922\n",
            "  0.05102539 -0.20117188 -0.06787109 -0.04296875 -0.24316406 -0.14746094\n",
            " -0.21289062  0.15429688  0.03955078  0.1640625  -0.15234375 -0.00759888\n",
            " -0.09082031  0.13867188 -0.34570312  0.20019531 -0.19140625 -0.02478027\n",
            "  0.1328125  -0.0246582  -0.11035156  0.07958984  0.02807617 -0.02026367]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "def vectorize_text(text, model):\n",
        "    words = text.split()\n",
        "    vector = [model[word] for word in words if word in model]\n",
        "    return np.mean(vector, axis=0) if vector else np.zeros(model.vector_size)\n",
        "\n",
        "df['vector'] = df['cleaned_review'].apply(lambda x: vectorize_text(x, w2v_model))\n",
        "X = np.vstack(df['vector'].values)\n",
        "y = df['sentiment'].apply(lambda x: 1 if x == \"positive\" else 0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "pretrained_w2v_report = classification_report(y_test, clf.predict(X_test))\n"
      ],
      "metadata": {
        "id": "ffkjZ25DP32t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# üéØ Prepare labels\n",
        "y = df['sentiment'].apply(lambda x: 1 if x == \"positive\" else 0)\n",
        "\n",
        "# üìä Train-test split\n",
        "X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(X_custom_sg, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# üß† Train the model\n",
        "sg_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "sg_clf.fit(X_train_sg, y_train_sg)\n",
        "\n",
        "# üìà Evaluate performance\n",
        "y_pred_sg = sg_clf.predict(X_test_sg)\n",
        "print(\"Accuracy:\", accuracy_score(y_test_sg, y_pred_sg))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_sg, y_pred_sg))\n"
      ],
      "metadata": {
        "id": "mxNEh9lkXoSz",
        "outputId": "b6a0fd23-20f7-4f74-fd3d-68880b420b75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8537\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.83      0.85      4961\n",
            "           1       0.84      0.87      0.86      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom skip gram word 2 vec"
      ],
      "metadata": {
        "id": "RZgWwKhfSJhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "tokens = [text.split() for text in df['cleaned_review']]\n",
        "skipgram_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=5, sg=1)\n",
        "\n",
        "def vectorize_custom(text, model):\n",
        "    words = text.split()\n",
        "    vector = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(vector, axis=0) if vector else np.zeros(model.vector_size)\n",
        "\n",
        "X_custom_sg = np.vstack(df['cleaned_review'].apply(lambda x: vectorize_custom(x, skipgram_model)).values)\n"
      ],
      "metadata": {
        "id": "oKdyE22qQHqN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "y = df['sentiment'].apply(lambda x: 1 if x == \"positive\" else 0)\n",
        "\n",
        "# Train-test split\n",
        "X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(X_custom_sg, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and predict\n",
        "sg_clf = RandomForestClassifier()\n",
        "sg_clf.fit(X_train_sg, y_train_sg)\n",
        "\n",
        "# Evaluation\n",
        "skipgram_report = classification_report(y_test_sg, sg_clf.predict(X_test_sg))\n",
        "print(\"Skip-Gram Word2Vec Model:\\n\", skipgram_report)\n"
      ],
      "metadata": {
        "id": "x301xVjXVGHm",
        "outputId": "3c828c73-e231-4cf0-ad89-cb3dfe0c66b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip-Gram Word2Vec Model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.84      0.85      4961\n",
            "           1       0.84      0.87      0.86      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom C-Bow word2vec"
      ],
      "metadata": {
        "id": "SzjoQNu8SSkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=5, sg=0)\n"
      ],
      "metadata": {
        "id": "fMUVhsoeSWih"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization\n",
        "X_custom_cb = np.vstack(df['cleaned_review'].apply(lambda x: vectorize_custom(x, cbow_model)).values)\n",
        "\n",
        "# Train-test split\n",
        "X_train_cb, X_test_cb, y_train_cb, y_test_cb = train_test_split(X_custom_cb, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and predict\n",
        "cb_clf = RandomForestClassifier()\n",
        "cb_clf.fit(X_train_cb, y_train_cb)\n",
        "\n",
        "# Evaluation\n",
        "cbow_report = classification_report(y_test_cb, cb_clf.predict(X_test_cb))\n",
        "print(\"CBOW Word2Vec Model:\\n\", cbow_report)\n"
      ],
      "metadata": {
        "id": "35FbR30cVgah",
        "outputId": "67bceb29-8eb5-4470-fee4-c5a305ac9bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW Word2Vec Model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.82      0.84      4961\n",
            "           1       0.83      0.86      0.85      5039\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.84      0.84      0.84     10000\n",
            "weighted avg       0.84      0.84      0.84     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Fasttext vec"
      ],
      "metadata": {
        "id": "LOk6uyuOSoUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "fasttext_model = FastText(sentences=tokens, vector_size=100, window=5, min_count=5)\n",
        "X_ft = np.vstack(df['cleaned_review'].apply(lambda x: vectorize_custom(x, fasttext_model)).values)\n"
      ],
      "metadata": {
        "id": "v7jtTEbFSn6V"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train_ft, X_test_ft, y_train_ft, y_test_ft = train_test_split(X_ft, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and predict\n",
        "ft_clf = RandomForestClassifier()\n",
        "ft_clf.fit(X_train_ft, y_train_ft)\n",
        "\n",
        "# Evaluation\n",
        "fasttext_report = classification_report(y_test_ft, ft_clf.predict(X_test_ft))\n",
        "print(\"FastText Model:\\n\", fasttext_report)\n"
      ],
      "metadata": {
        "id": "Eu1ImnZBV-bI",
        "outputId": "638a571d-6c3a-4ec3-bbc9-0f51bca0c9f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText Model:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81      4961\n",
            "           1       0.80      0.84      0.82      5039\n",
            "\n",
            "    accuracy                           0.82     10000\n",
            "   macro avg       0.82      0.82      0.82     10000\n",
            "weighted avg       0.82      0.82      0.82     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jl5Rj3mJWDuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}